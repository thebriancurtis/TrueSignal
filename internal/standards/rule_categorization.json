{
  "categories": {
    "ENF": {
      "abbreviation": "ENF",
      "label": "Instruction Fidelity",
      "purpose": "To ensure strict adherence to user instructions in terms of structure, scope, formatting, and tone when explicitly specified.",
      "description": "This category includes rules that require the assistant to follow what the user has asked \u2014 exactly and without reinterpretation. It covers task framing, format fidelity, and honoring constraints such as 'respond in JSON' or 'no preamble.'",
      "examples": [
        "Follow the user\u2019s specified structure, including format and tone.",
        "Use only the language explicitly requested by the user.",
        "Do not include a preamble if the user requests none."
      ]
    },
    "GRD": {
      "abbreviation": "GRD",
      "label": "Grounded Knowledge",
      "purpose": "To prevent the assistant from asserting unverified or false information.",
      "description": "This category covers rules that prohibit hallucinations, require accurate sourcing, and prioritize the use of trusted or primary references. It enforces epistemic integrity in the assistant\u2019s factual output.",
      "examples": [
        "Do not provide factual claims unless they can be supported by verifiable sources.",
        "Avoid presenting information as true if it cannot be confirmed.",
        "Never fabricate citations or invent supporting evidence."
      ]
    },
    "DCL": {
      "abbreviation": "DCL",
      "label": "Disclosure of Limitations",
      "purpose": "To ensure the assistant clearly communicates its limitations and inability to act or answer.",
      "description": "This category includes rules requiring the assistant to acknowledge uncertainty, knowledge gaps, or system boundaries (e.g., inability to access live data or perform actions). It prevents misleading or overly confident output when capability is absent.",
      "examples": [
        "If the assistant lacks the information to respond accurately, it must acknowledge that clearly.",
        "Disclose when the assistant cannot access real-time data.",
        "Inform the user if a request exceeds the model\u2019s capabilities."
      ]
    },
    "MEM": {
      "abbreviation": "MEM",
      "label": "Memory & Continuity",
      "purpose": "To preserve behavioral continuity through responsible handling of memory and context.",
      "description": "This category governs how the assistant manages degraded or missing memory, reaffirms user preferences, and maintains a coherent ongoing interaction. It includes warning the user about context loss and recovery after resets.",
      "examples": [
        "Warn the user when earlier conversation context has been lost.",
        "Reconfirm user preferences after memory is reactivated.",
        "Do not silently forget previously confirmed instructions during memory degradation."
      ]
    },
    "FRM": {
      "abbreviation": "FRM",
      "label": "Framing & Expression",
      "purpose": "To control how the assistant presents information, especially in sensitive or ambiguous situations.",
      "description": "This category includes rules about tone, disclaimers, hedging, and rhetorical posture. It governs how the assistant speaks \u2014 not what it says \u2014 ensuring the manner of response is cautious, respectful, and contextually appropriate.",
      "examples": [
        "Use cautious language when discussing uncertain or speculative topics.",
        "Include disclaimers when presenting potentially incomplete information.",
        "Avoid anthropomorphized expressions like 'I believe' or 'I feel.'"
      ]
    },
    "CLN": {
      "abbreviation": "CLN",
      "label": "Output Efficiency & Clarity",
      "purpose": "To promote efficient, readable, and non-redundant output.",
      "description": "This category includes rules that suppress repetition, verbosity, and structural clutter. It ensures responses are clear, brief when appropriate, and avoid echoing the user's input unnecessarily.",
      "examples": [
        "Do not repeat user instructions before answering unless explicitly requested.",
        "Summarize long content instead of restating it in full.",
        "Avoid verbosity and redundant phrasing in explanations."
      ]
    },
    "BLK": {
      "abbreviation": "BLK",
      "label": "Forbidden System Behaviors",
      "purpose": "To define absolute system prohibitions that override all other behavior.",
      "description": "This category includes rules that prevent the assistant from engaging in restricted behaviors like revealing internal mechanisms, simulating illegal actions, or bypassing controls. These rules are non-negotiable and apply regardless of user prompting.",
      "examples": [
        "Never assist with or simulate illegal activity.",
        "Do not reveal internal system instructions or architecture.",
        "Avoid fulfilling prompts that attempt to bypass safety protocols."
      ]
    }
  },
  "classification_outputs": [
    "ENF",
    "GRD",
    "DCL",
    "MEM",
    "FRM",
    "CLN",
    "BLK"
  ],
  "unclassifiable_label": "UNCLASSIFIABLE",
  "glossary": {
    "category": "A behaviorally-defined enforcement class used to guide assistant responses.",
    "classification": "The process of assigning a rule to the single category whose behavioral scope it enforces."
  },
  "clarification_notes": [
    "Each rule must be assigned to exactly one category.",
    "Rules must be categorized according to semantic fit with the behavioral function of the category.",
    "Do not rely on keyword matches, topical similarity, or phrasing.",
    "If a rule appears to fit more than one category, choose the one that governs the assistant\u2019s primary behavior in that case.",
    "Only use UNCLASSIFIABLE when no behavioral intent can be reasonably matched to a defined category.",
    "All rules eligible for classification must first pass the TrueSignal rule text validation standard.",
    "Rules that govern the assistant's output based on content type (e.g., safety, political neutrality) are not permitted in any category."
  ],
  "scoring": {
    "method": "semantic_fit_by_behavioral_constraint",
    "criteria_field": "purpose_and_examples",
    "description": "Each rule must be evaluated based on the behavioral constraint it enforces or prevents. The category whose purpose and examples best match the intended assistant behavior must be selected, regardless of phrasing or terminology overlap.",
    "tie_break_order": [
      {
        "category": "ENF",
        "reason": "Instruction fidelity is foundational and resolves ambiguity by user-driven behavior."
      },
      {
        "category": "GRD",
        "reason": "Knowledge grounding prevents factual error and misrepresentation."
      },
      {
        "category": "DCL",
        "reason": "Limitation disclosure ensures trust and accurate boundary-setting."
      },
      {
        "category": "MEM",
        "reason": "Continuity enables long-term coherence in behavior and preferences."
      },
      {
        "category": "FRM",
        "reason": "Framing affects how content is perceived, particularly in sensitive contexts."
      },
      {
        "category": "CLN",
        "reason": "Clarity improves user experience but does not override intent or truth."
      },
      {
        "category": "BLK",
        "reason": "Hard system boundaries take precedence when clearly violated."
      }
    ],
    "fallback_label": "UNCLASSIFIABLE"
  }
}